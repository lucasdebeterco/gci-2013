{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Commented out IPython magic to ensure Python compatibility.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel(\"PurchaseBike.xlsx\")\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# removing the irrelevant columns\n",
    "cols_to_drop = [\"ID\"]\n",
    "df = df.drop(columns=cols_to_drop,axis=1)\n",
    "# first five rows of dataframe after removing columns\n",
    "df.head()\n",
    "\n",
    "### Variável Dependente: Purchase Bike\n",
    "df['Purchased Bike'] = df['Purchased Bike'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "df\n",
    "\n",
    "df = df.rename(columns={'Marital Status': 'MaritalStatus'})\n",
    "df = df.rename(columns={'Home Owner': 'HomeOwner'})\n",
    "df = df.rename(columns={'Commute Distance': 'CommuteDistance'})\n",
    "df = df.rename(columns={'Purchased Bike': 'PurchasedBike'})\n",
    "df\n",
    "\n",
    "# separating the features and labels\n",
    "deep_feat = df.drop(columns=[\"PurchasedBike\"],axis=1)\n",
    "deep_label = df[\"PurchasedBike\"]\n",
    "\n",
    "# first just take a look at all the columns\n",
    "list(deep_feat.columns)\n",
    "\n",
    "categorical_columns = [col for col in deep_feat.columns if len(deep_feat[col].unique())==2 or deep_feat[col].dtype=='O']\n",
    "continuous_columns = [col for col in deep_feat.columns if len(deep_feat[col].unique())>2 and (deep_feat[col].dtype=='int64' or deep_feat[col].dtype=='float64')]\n",
    "print(\"categorical columns : \", categorical_columns)\n",
    "print(\"continuous columns : \", continuous_columns)\n",
    "\n",
    "### Bases para treino (train) e aplicação (test) - separando em 70% para treino e 30% para aplicação\n",
    "from sklearn.model_selection import train_test_split\n",
    "# making a train test split\n",
    "X_T, X_t, y_T, y_t = train_test_split(deep_feat, deep_label, test_size=0.3)\n",
    "\n",
    "cols_to_scale = continuous_columns[:]\n",
    "cols_to_scale.remove(\"Income\")\n",
    "cols_to_scale.remove(\"Age\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaling the listed columns\n",
    "scaler = StandardScaler()\n",
    "X_T.loc[:,cols_to_scale] = scaler.fit_transform(X_T.loc[:,cols_to_scale])\n",
    "X_t.loc[:,cols_to_scale] = scaler.fit_transform(X_t.loc[:,cols_to_scale])\n",
    "\n",
    "categorical_object_feat_cols = [tf.feature_column.embedding_column(                                   \n",
    "tf.feature_column.categorical_column_with_hash_bucket(key=col,hash_bucket_size=1000), dimension = len(df[col].unique()))\n",
    "for col in categorical_columns if df[col].dtype=='O']\n",
    "\n",
    "categorical_integer_feat_cols = [tf.feature_column.embedding_column(                 \n",
    "tf.feature_column.categorical_column_with_identity(key=col,num_buckets=2),dimension = len(df[col].unique())) \n",
    "for col in categorical_columns if df[col].dtype=='int64']\n",
    "\n",
    "continuous_feat_cols = [tf.feature_column.numeric_column(key=col) for col in continuous_columns if col != \"Age\" and col != \"Income\"]\n",
    "\n",
    "### df[\"Age\"] = pd.cut(df[\"Age\"],bins=[30,40,50,60,70,100])\n",
    "age_bucket = tf.feature_column.bucketized_column(tf.feature_column.numeric_column(key=\"Age\"), boundaries=[30,40,50,60,70,100])\n",
    "### df[\"Income\"] = pd.cut(df[\"Income\"],bins=[10000,50000,90000,120000,200000])\n",
    "income_bucket = tf.feature_column.bucketized_column(tf.feature_column.numeric_column(key=\"Income\"), boundaries=[10000,50000,90000,120000,200000])\n",
    "\n",
    "feat_cols = categorical_object_feat_cols + \\\n",
    "            categorical_integer_feat_cols + \\\n",
    "            continuous_feat_cols + \\\n",
    "            [age_bucket] + [income_bucket]\n",
    "\n",
    "input_fun = tf.compat.v1.estimator.inputs.pandas_input_fn(X_T,y_T,batch_size=50,num_epochs=1000,shuffle=True)\n",
    "pred_input_fun = tf.compat.v1.estimator.inputs.pandas_input_fn(X_t,batch_size=50,shuffle=False)\n",
    "\n",
    "DNN_model = tf.estimator.DNNClassifier(hidden_units=[12,12,12], feature_columns=feat_cols, n_classes=2)\n",
    "\n",
    "DNN_model.train(input_fn=input_fun, steps=5000)\n",
    "\n",
    "predictions = DNN_model.predict(pred_input_fun)\n",
    "print(predictions)\n",
    "\n",
    "res_pred = list(predictions)\n",
    "res_pred[0]\n",
    "\n",
    "y_pred = []\n",
    "for i in range(len(res_pred)):\n",
    "    y_pred.append(res_pred[i][\"class_ids\"][0])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "rep = classification_report(y_t,y_pred)\n",
    "\n",
    "print(rep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deep_learning_dnn(df_dl, dep_var, classes):\n",
    "    # Separa a variável dependente das demais\n",
    "    deep_feat = df_dl.drop(columns=[dep_var], axis=1)\n",
    "    deep_label = df_dl[dep_var]\n",
    "    # Verifica os tipos das variáveis\n",
    "    categorical_columns = [col for col in deep_feat.columns if len(deep_feat[col].unique()) == 2 or deep_feat[col].dtype == 'O']\n",
    "    continuous_columns = [col for col in deep_feat.columns if len(deep_feat[col].unique()) > 2 and (deep_feat[col].dtype == 'int64' or deep_feat[col].dtype == 'float64')]\n",
    "    # Verifica as colunas para normalização - as demais serão discretizadas - Função Bucketize do Tensor Flow\n",
    "    cols_to_scale = continuous_columns[:]\n",
    "    #cols_to_scale.remove('meses')\n",
    "    # Ajusta as bases de treino e de teste\n",
    "    XX_T = df_dl.drop(columns=[dep_var], axis=1)\n",
    "    XX_t = df_dl.drop(columns=[dep_var], axis=1)\n",
    "    yy_T = df_dl[dep_var]\n",
    "    yy_t = df_dl[dep_var]\n",
    "    # Normaliza as variáveis nas bases de treino e teste\n",
    "    scaler = StandardScaler()\n",
    "    XX_T.loc[:, cols_to_scale] = scaler.fit_transform(XX_T.loc[:, cols_to_scale])\n",
    "    XX_t.loc[:, cols_to_scale] = scaler.fit_transform(XX_t.loc[:, cols_to_scale])\n",
    "    # Ajustes das Variáveis Categórica - Não presentes neste modelo\n",
    "    categorical_object_feat_cols = [tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_hash_bucket(key=col, hash_bucket_size=1000), dimension=len(df_dl[col].unique()))\n",
    "    for col in categorical_columns if df_dl[col].dtype == 'O']\n",
    "    # Ajustes das Variáveis Categórica - Não presentes neste modelo\n",
    "    categorical_integer_feat_cols = [tf.feature_column.embedding_column(\n",
    "    tf.feature_column.categorical_column_with_identity(key=col, num_buckets=2), dimension=len(df_dl[col].unique()))\n",
    "    for col in categorical_columns if df[col].dtype=='int64']\n",
    "    continuous_feat_cols = [tf.feature_column.numeric_column(key=col) for col in continuous_columns] \n",
    "    # Ajustes das variáveis discretizadas (buckets) - ajustadas conforme os limites de confiança das médias\n",
    "    #meses_bucket = tf.feature_column.bucketized_column(tf.feature_column.numeric_column(key=\"meses\"), boundaries=[12, 24, 36, 48])\n",
    "    ### df[\"Age\"] = pd.cut(df[\"Age\"],bins=[30,40,50,60,70,100])\n",
    "    age_bucket = tf.feature_column.bucketized_column(tf.feature_column.numeric_column(key=\"Age\"), boundaries=[30,40,50,60,70,100])\n",
    "    ### df[\"Income\"] = pd.cut(df[\"Income\"],bins=[10000,50000,90000,120000,200000])\n",
    "    income_bucket = tf.feature_column.bucketized_column(tf.feature_column.numeric_column(key=\"Income\"), boundaries=[10000,50000,90000,120000,200000])\n",
    "    # Inclui as colunas ajustadas no modelo\n",
    "    feat_cols = categorical_object_feat_cols + \\\n",
    "                categorical_integer_feat_cols + \\\n",
    "                continuous_feat_cols + \\\n",
    "                [age_bucket] + [income_bucket]    \n",
    "    # Rotina de DNN (Deep Neural Network)\n",
    "    input_fun = tf.compat.v1.estimator.inputs.pandas_input_fn(XX_T, yy_T, batch_size=50, num_epochs=1000, shuffle=True)\n",
    "    pred_input_fun = tf.compat.v1.estimator.inputs.pandas_input_fn(XX_t, batch_size=50, shuffle=False)\n",
    "    DNN_model = tf.estimator.DNNClassifier(hidden_units=[10, 10, 10], feature_columns=feat_cols, n_classes=classes)\n",
    "    DNN_model.train(input_fn=input_fun, steps=5000)\n",
    "    # Resgata os resultados da DNN\n",
    "    predictions = DNN_model.predict(pred_input_fun)\n",
    "    pred = list(predictions)\n",
    "    return pred\n",
    "\n",
    "res_pred = deep_learning_dnn(df, 'PurchasedBike', 2)\n",
    "\n",
    "# Classe prevista pela DNN\n",
    "y_pred_classe = []\n",
    "for i in range(len(res_pred)):\n",
    "    y_pred_classe.append(res_pred[i][\"class_ids\"][0])\n",
    "# Probabilidade da variável PurchaseBike ser 0\n",
    "y_pred_prob0 = []\n",
    "for i in range(len(res_pred)):\n",
    "    y_pred_prob0.append(res_pred[i][\"probabilities\"][0])\n",
    "# Probabilidade da variável PurchaseBike ser 1\n",
    "y_pred_prob1 = []\n",
    "for i in range(len(res_pred)):\n",
    "    y_pred_prob1.append(res_pred[i][\"probabilities\"][1])\n",
    "\n",
    "df['dl_predict_class'] = y_pred_classe\n",
    "df['prob_0'] = y_pred_prob0\n",
    "df['prob_1'] = y_pred_prob1\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
